[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "T. Jalkanen; Isoconversional kinetic analysis for determining the rate of cross-linking for Pt and peroxide cure silicone rubbers; Thermochimica Acta, 703, 178982 (2021)\n\n\n\nR. Nave, T. Jalkanen, C. Talling, M. Kaneko, S. Matsuki, and J. Höchel; The effect of drug content reduction on the in vitro and in vivo properties of levonorgestrel-releasing intravaginal rings; Journal of Pharmaceutical Sciences, 107, 1020-1027 (2018)\n\n\n\nL. Jäppinen, T. Jalkanen, B. Sieber, A. Addad, M. Heinonen, E. Kukk, I. Radevici, P. Paturi, M. Peurla, M.-A. Shahbazi, H. A. Santos, R. Boukherroub, H. Santos, M. Lastusaari, and J. Salonen; Enhanced photoluminescence in acetylene-treated ZnO nanorods; Nanoscale Research Letters 11:413 (2016)\n\n\n\nT. Jalkanen, A. Määttänen, E. Mäkilä, J. Tuura, M. Kaasalainen, V.-P. Lehto, P. Ihalainen, J. Peltonen, and J. Salonen; Fabrication of porous silicon based humidity sensing elements on paper; Journal of Sensors, Article ID 927396\n\n\n\nT. Jalkanen, V. Torres-Costa, E. Mäkilä, R. Koda, T. Sakka, Y.-H. Ogata, and J. Salonen; Selective optical response of hydrolytically stable stratified Si rugate mirrors to liquid infiltration; ACS Applied Materials & Interfaces 6, 2884-2892 (2014)\n\n\n\nT. Jalkanen, E. Mäkilä, A. Määttänen, J. Tuura, M. Kaasalainen, V.-P. Lehto, P. Ihalainen, J. Peltonen, and J. Salonen; Porous silicon micro- and nanoparticles for printed humidity sensors; Applied Physics Letters 101, 263110 (2012)\nT. Jalkanen, E. Mäkilä, Y.-I. Suzuki, T. Urata, K. Fukami, T. Sakka, J. Salonen, and Y. H. Ogata; Studies on chemical modification of porous silicon-based graded-index optical microcavities for improved stability under alkaline conditions; Advanced Functional Materials 22, 3890-3898 (2012)\nT. Jalkanen, E. Mäkilä, T. Sakka, J. Salonen, and Y. H. Ogata; Thermally promoted addition of undecylenic acid on thermally hydrocarbonized porous silicon optical reflectors; Nanoscale Research Letters 7:311 (2012)\n\n\n\nT. Jalkanen, J. Salonen, V. Torres-Costa, K. Fukami, T. Sakka, and Y.H. Ogata; Structural considerations on multistopband mesoporous silicon rugate filters prepared for gas sensing purposes; Optics Express 19, 13291-13305 (2011)\n\n\n\nT. Jalkanen, J. Tuura, E. Mäkilä, and J. Salonen; Electro-optical porous silicon gas sensor with enhanced selectivity; Sensors and Actuators B 147, 100-104 (2010)\n\n\n\nT. Jalkanen, V. Torres-Costa, J. Salonen, M. Björkqvist, E. Mäkilä, J.M. Martínez-Duart, and V.-P. Lehto; Optical gas sensing properties of thermally hydrocarbonized porous silicon Bragg reflectors; Optics Express 17, 5446-5456 (2009)\nV. Torres-Costa, J. Salonen, T. M. Jalkanen, V.-P. Lehto, R.J. Martín-Palma, and J.M. Martínez-Duart; Carbonization of porous silicon optical gas sensors for enhanced stability and sensitivity; Physica Status Solidi A 206, 1306-1308 (2009)\nM. Björkqvist, J. Salonen, J. Tuura, T. Jalkanen, and V.-P. Lehto; Detecting amine vapours with thermally carbonized porous silicon gas sensor; Physica Status Solidi C 6, 1769-1772 (2009)"
  },
  {
    "objectID": "publications.html#peer-reviewed-publications",
    "href": "publications.html#peer-reviewed-publications",
    "title": "Publications",
    "section": "",
    "text": "T. Jalkanen; Isoconversional kinetic analysis for determining the rate of cross-linking for Pt and peroxide cure silicone rubbers; Thermochimica Acta, 703, 178982 (2021)\n\n\n\nR. Nave, T. Jalkanen, C. Talling, M. Kaneko, S. Matsuki, and J. Höchel; The effect of drug content reduction on the in vitro and in vivo properties of levonorgestrel-releasing intravaginal rings; Journal of Pharmaceutical Sciences, 107, 1020-1027 (2018)\n\n\n\nL. Jäppinen, T. Jalkanen, B. Sieber, A. Addad, M. Heinonen, E. Kukk, I. Radevici, P. Paturi, M. Peurla, M.-A. Shahbazi, H. A. Santos, R. Boukherroub, H. Santos, M. Lastusaari, and J. Salonen; Enhanced photoluminescence in acetylene-treated ZnO nanorods; Nanoscale Research Letters 11:413 (2016)\n\n\n\nT. Jalkanen, A. Määttänen, E. Mäkilä, J. Tuura, M. Kaasalainen, V.-P. Lehto, P. Ihalainen, J. Peltonen, and J. Salonen; Fabrication of porous silicon based humidity sensing elements on paper; Journal of Sensors, Article ID 927396\n\n\n\nT. Jalkanen, V. Torres-Costa, E. Mäkilä, R. Koda, T. Sakka, Y.-H. Ogata, and J. Salonen; Selective optical response of hydrolytically stable stratified Si rugate mirrors to liquid infiltration; ACS Applied Materials & Interfaces 6, 2884-2892 (2014)\n\n\n\nT. Jalkanen, E. Mäkilä, A. Määttänen, J. Tuura, M. Kaasalainen, V.-P. Lehto, P. Ihalainen, J. Peltonen, and J. Salonen; Porous silicon micro- and nanoparticles for printed humidity sensors; Applied Physics Letters 101, 263110 (2012)\nT. Jalkanen, E. Mäkilä, Y.-I. Suzuki, T. Urata, K. Fukami, T. Sakka, J. Salonen, and Y. H. Ogata; Studies on chemical modification of porous silicon-based graded-index optical microcavities for improved stability under alkaline conditions; Advanced Functional Materials 22, 3890-3898 (2012)\nT. Jalkanen, E. Mäkilä, T. Sakka, J. Salonen, and Y. H. Ogata; Thermally promoted addition of undecylenic acid on thermally hydrocarbonized porous silicon optical reflectors; Nanoscale Research Letters 7:311 (2012)\n\n\n\nT. Jalkanen, J. Salonen, V. Torres-Costa, K. Fukami, T. Sakka, and Y.H. Ogata; Structural considerations on multistopband mesoporous silicon rugate filters prepared for gas sensing purposes; Optics Express 19, 13291-13305 (2011)\n\n\n\nT. Jalkanen, J. Tuura, E. Mäkilä, and J. Salonen; Electro-optical porous silicon gas sensor with enhanced selectivity; Sensors and Actuators B 147, 100-104 (2010)\n\n\n\nT. Jalkanen, V. Torres-Costa, J. Salonen, M. Björkqvist, E. Mäkilä, J.M. Martínez-Duart, and V.-P. Lehto; Optical gas sensing properties of thermally hydrocarbonized porous silicon Bragg reflectors; Optics Express 17, 5446-5456 (2009)\nV. Torres-Costa, J. Salonen, T. M. Jalkanen, V.-P. Lehto, R.J. Martín-Palma, and J.M. Martínez-Duart; Carbonization of porous silicon optical gas sensors for enhanced stability and sensitivity; Physica Status Solidi A 206, 1306-1308 (2009)\nM. Björkqvist, J. Salonen, J. Tuura, T. Jalkanen, and V.-P. Lehto; Detecting amine vapours with thermally carbonized porous silicon gas sensor; Physica Status Solidi C 6, 1769-1772 (2009)"
  },
  {
    "objectID": "publications.html#peer-reviewed-conference-proceedings",
    "href": "publications.html#peer-reviewed-conference-proceedings",
    "title": "Publications",
    "section": "Peer-Reviewed Conference proceedings",
    "text": "Peer-Reviewed Conference proceedings\n\n\n\n\n\n\n\n\n\n\nTitle\nAuthors\nInfo\n\n\n\n\nThermal carbonization of porous silicon: The current status and recent applications\nJ. Salonen, M. Kaasalainen, O.-P. Rauhala, L. Lassila, M. Hakamies, T. Jalkanen, R-. Hahn, P. Schmuki, and E. Mäkilä\nECS Transactions 69, 167-176 (2015)\n\n\nFunctionalization of thermally carbonized porous silicon optical multilayer structures for sensing applications\nJ. Salonen, E. Mäkilä, T. Urata, V. Torres-Costa, and T. Jalkanen\nECS Transactions 58, 63-70 (2013)"
  },
  {
    "objectID": "publications.html#open-data",
    "href": "publications.html#open-data",
    "title": "Publications",
    "section": "Open Data",
    "text": "Open Data\n\n\n\n\n\nTitle\nAuthors\nInfo\n\n\n\n\nThe rate of cross-linking for Pt and peroxide cure silicone rubbers\nT. Jalkanen\nMendeley Data, V1 (2021)"
  },
  {
    "objectID": "publications.html#thesis",
    "href": "publications.html#thesis",
    "title": "Publications",
    "section": "Thesis",
    "text": "Thesis\n\n\n\n\n\nDegree\nMajor\nTitle\nUniversity\nLink\n\n\n\n\nPhD\nPhysics\nPorous silicon optical filters in gas sensing applications\nUniversity of Turku (2012)\nElectronic version available at: https://urn.fi/URN:ISBN:978-951-29-5186-4"
  },
  {
    "objectID": "posts/05-docker.html",
    "href": "posts/05-docker.html",
    "title": "Using Docker for Shiny Apps",
    "section": "",
    "text": "Shiny apps are a great way to provide an interactive user interface for exploring data, running models, simulations, and much more. However, writing a Shiny app on your local computer is one thing, but providing it to a customer or colleague in a scalable way requires something more. Enter Docker.\nWhat is Docker? Docker provides a way to run your Shiny app inside something called a container, which is basically an isolated virtual environment (read more here). This allows you to share your app via cloud providers such as AWS and Azure. You can also run the container on a local machine, without needing to worry about R installations and other dependencies.\nSounds great. And the best thing is that it is not that difficult to containerize your Shiny app. Let’s see what is needed."
  },
  {
    "objectID": "posts/05-docker.html#installing-docker",
    "href": "posts/05-docker.html#installing-docker",
    "title": "Using Docker for Shiny Apps",
    "section": "Installing Docker",
    "text": "Installing Docker\nYou will need to install Docker Desktop to get the job done. The installation depends on your operating system, but it should be easy enough for Mac OS, Windows, and Linux as well. The website has detailed instructions to guide you through the process."
  },
  {
    "objectID": "posts/05-docker.html#building-a-docker-image",
    "href": "posts/05-docker.html#building-a-docker-image",
    "title": "Using Docker for Shiny Apps",
    "section": "Building a Docker image",
    "text": "Building a Docker image\nNow that Docker is up and running we need to create something called a Dockerfile. It is needed for building a Docker image of our Shiny app. What is an image then? Image is basically a blueprint for your container. It tells what code should run inside the container, and which dependencies the code has starting from the operating system (OS), programming language versions, packages etc. The container itself, is basically a running instance of the said image.\nSpecifying everything, starting from the OS, for the Docker image may sound intimidating. Luckily, we do not need to start from zero. Rather, we can build on top of existing images. There are thousands of images readily available in Docker Hub. For R users, several useful images are listed under rocker. For Shiny apps, rocker/shiny and rocker/shiny-verse are good starting points, which contain everything needed for building a simple Shiny app.\nLet’s run through the process of building a custom Docker image on top of an existing rocker image. You need to do three things:\n\nCreate an empty file called Dockerfile. If the code for your Shiny app is located inside a folder called app, then the Dockerfile should be saved in the parent folder (so Dockerfile inside . and Shiny code inside ./app)\nWrite the following commands to the Dockerfile and save it:\n\nFROM rocker/shiny-verse:latest\n\nCOPY /app /srv/shiny-server/\n\nEXPOSE 3838\n\nNavigate to the folder where your Dockerfile is located with the command line tool of your choice and build the image based on the Dockerfile with the following command:\n\n$ docker build --tag my-app:latest .\nFor phase 3), you can use for example the Terminal which is available in either RStudio or Visual Studio Code. The advantage of using a built-in terminal from e.g. Visual Studio Code is that the terminal prompt is already opened in your working directory where you have saved your Dockerfile. Visual Studio Code also provides help for writing Dockerfiles via the Docker extension.\nLet’s take a closer look at the contents of that Dockerfile and what is actually happening. The first line of code tells Docker to build the image on top of a pre-existing image called rocker/shiny-verse. If that is not available locally, then Docker will search and pull the image from Docker Hub. The next line of code tells Docker to copy the contents of the local app folder on your computer, and duplicate it inside the Docker image. There it will place the contents behind the following path ./srv/shiny-server/. The app folder should contain all the code for your Shiny app. The last line of code declares that port 3838 of the container created based on this image should be exposed to the outside environment.\nNow if we wish to run a container based on the image locally on our computer, we can do so with the following command:\n$ docker run -p 3838:3838 -d --rm --name mycontainer my-app:latest\nAbove we are telling Docker to run a container based on the my-app:latest image. The container is by default isolated from your computer, so the flag -p 3838:3838 tells Docker that we want to connect from port 3838 to port 3838 in the container. The flags -d and --rm are for running the container in detached mode (logs are not printed to terminal), and for removing the container once it is stopped, respectively. Finally, with the --name flag, we give the container a name, which in this case is mycontainer.\nAfter running the command, you should find your containerized Shiny app by visiting localhost:3838 on your web browser.\n\n\n\n\n\nShiny app running locally inside a container.\n\n\n\n\nOnce you are done using the app, you can shutdown the container with:\n$ docker stop mycontainer\nThis will also remove the container since we used the --rm flag when starting the container."
  },
  {
    "objectID": "posts/05-docker.html#installing-r-packages",
    "href": "posts/05-docker.html#installing-r-packages",
    "title": "Using Docker for Shiny Apps",
    "section": "Installing R packages",
    "text": "Installing R packages\nThe readily available rocker/shiny and rocker/shiny-verse images include many R-packages by default. You can explore the information on different rocker images and their contents here. However, if you are preparing a Shiny app for a specific purpose, chances are that at some point you will need to install R packages not included in the above mentioned rocker images.\nLet’s say we would like to use the {plotly} package in our app for some interactive plots. This is quite easy to achieve. We need to add one more command into our Dockerfile:\nFROM rocker/shiny-verse:latest\n\n## install necessary r-libraries\nRUN R -e \"install.packages('plotly', repos = 'https://cloud.r-project.org')\"\n\nCOPY /app /srv/shiny-server/\n\nEXPOSE 3838\nThe RUN R -e \"install.packages(...)\" command does the heavy lifting. All that is left for us to do is to specify the package name, and a CRAN mirror which is detailed under the repos argument. Now, we just need to rebuild the Docker image with docker build, and plotly will be available inside the containers created based on the image.\n\nInstalling custom R packages\nOk, so installing CRAN packages is pretty easy. But what should you do in case you want to include a custom package (for example, an internal company package or something you wrote for yourself)? Let’s say we have written an R package called myawesomepkg, and want to use it. We have saved a copy of our package with devtools::build() inside a folder called pkgs. Installing the package locally in a regular R session is easy (just run install.packages('pkgs/myawesomepkg_0.0.1.tar.gz', repos = NULL)). If we want make the same package available inside a Docker container, we first need to copy the package tar.gz-file inside the image file system, and then run the install command. The install.packages() command can be included inside a regular R script. Let’s modify our Dockerfile one last time:\nFROM rocker/shiny-verse:latest\n\n## install necessary r-libraries\nRUN R -e \"install.packages('plotly', repos = 'https://cloud.r-project.org')\"\n\n## copy files for internal package installations\nCOPY /pkgs /pkgs\nCOPY /code /code\n\n## install R-packages\nRUN Rscript /code/install_packages.R\n\nCOPY /app /srv/shiny-server/\n\nEXPOSE 3838\nSo, now we are copying the ./pkgs/ and ./code/ folders from our local computer inside the image file system. The ./pkgs/ folder contains the tar.gz-file for our custom package, and the ./code/ folder contains an R-script (install_packages.R) with the needed R commands to install our custom package. Namely, one line: install.packages('pkgs/myawesomepkg_0.0.1.tar.gz', repos = NULL). That’s all we need to do. Our custom R package is now installed and available for our containerized Shiny app."
  },
  {
    "objectID": "posts/05-docker.html#launching-a-container-with-a-script-file",
    "href": "posts/05-docker.html#launching-a-container-with-a-script-file",
    "title": "Using Docker for Shiny Apps",
    "section": "Launching a container with a script file",
    "text": "Launching a container with a script file\nEarlier we saw that launching a container on your local computer is pretty straightforward. With a readily available image, one merely needs to write the following command to the command line:\n$ docker run -p 3838:3838 -d --rm --name mycontainer my-app:latest\nAs easy as this may seem, it is definitely not an ideal way to go if you want to make the container available to a customer or a less computer savvy colleague. Luckily, we can easily make a desktop script file which runs the command once double clicked, and opens the default browser. On Windows this is achievable via a batch file and on Mac OS a shell script file. The workflow is pretty similar.\n\nMaking a Windows batch file\nOpen the Notepad and save an empty file on your desktop with the name of e.g. LauchMyContainer.bat. Write the following contents inside the file:\ndocker run -p 3838:3838 -d --rm --name mycontainer my-app:latest\nstart \"\" http://localhost:3838/\nSave the file, and your done. When you now double click the bat-file it will first start the container, and then open the default internet browser on http://localhost:3838/. Once you are done working with the container you will need to stop it either via command line by writing docker stop mycontainer or by using the Docker Desktop GUI. You can also write another bat-file with the docker stop command.\n\n\nMaking a Mac OS shell script\nThe Mac OS equivalent to a Windows batch file is a file with a .command ending. It is basically a shell script which is executed in the terminal. To create one, navigate to your desktop inside the terminal app, and create an empty file by writing touch LauchMyContainer.command. Then you can open the file with a text editor, and add the following contents:\ndocker run -p 3838:3838 -d --rm --name mycontainer my-app:latest\nopen http://localhost:3838/\nFinally, you will probably need to run the following command in the terminal to adjust user permissions to make the file run.\n$ chmod u+x ./LauchMyContainer.command\nThis is assuming you are still inside the Desktop folder with the terminal. Now the script file will start the container and open the internet browser when double clicked."
  },
  {
    "objectID": "posts/05-docker.html#final-thoughts",
    "href": "posts/05-docker.html#final-thoughts",
    "title": "Using Docker for Shiny Apps",
    "section": "Final thoughts",
    "text": "Final thoughts\nSetting up Shiny apps to run inside a container is fairly straightforward, and allows you to share your work with other people in a scalable way. Once the app is running inside a container, you do not need to worry about the end users operating system or about any other software dependencies for that matter. The app will just work. A huge added bonus is that you can easily share containerized apps via cloud platforms as well. If you want to know more, Joe Torres has a great blog post detailing the steps needed for using Azure App service to host your Shiny app."
  },
  {
    "objectID": "posts/03-dual-axis-plots.html",
    "href": "posts/03-dual-axis-plots.html",
    "title": "Creating dual y-axis plots with ggplot2",
    "section": "",
    "text": "Every now and then I find myself in a situation where I need to create a plot with dual y-axes. However, this seems to happen rarely enough, that I end up frantically googling for advice. Now, creating that secondary y-axis ain’t exactly rocket science, but anyone who has been using ggplot2 will tell you that it is not trivial either. To save myself the trouble of having to look for advice in the future, I will show how it can be done with a few simple steps."
  },
  {
    "objectID": "posts/03-dual-axis-plots.html#re-organizing-the-plot-via-faceting",
    "href": "posts/03-dual-axis-plots.html#re-organizing-the-plot-via-faceting",
    "title": "Creating dual y-axis plots with ggplot2",
    "section": "Re-organizing the plot via faceting",
    "text": "Re-organizing the plot via faceting\nLet’s re-plot the data by using two y-axes. The easiest way to do this with ggplot2 is by using faceting.\n\n# plot wind and pressure\nmod_storms  %&gt;% \n  pivot_longer(cols = c(\"wind\", \"pressure\"), names_to = \"phenomenon\", values_to = \"measurement\") %&gt;%\n  ggplot(aes(x = date_hour, y = measurement, lty = phenomenon)) +\n  geom_line() +\n  facet_grid(phenomenon ~ ., scales = \"free_y\") +\n  labs(x = \"\", y = \"Measurement value\") +\n  theme(legend.position = \"top\")\n\n\n\n\nWind and air pressure changes for the Amy storm divided into two different plots.\n\n\n\n\nThe data is now faceted into two separate panes. This let’s us scale the y-axes independently, which already makes it easier to see that wind speeds are going up as the pressure is dropping. What we are still missing though, are the units on the y-axis. Let’s create one more iteration of this plot."
  },
  {
    "objectID": "posts/03-dual-axis-plots.html#two-y-axes",
    "href": "posts/03-dual-axis-plots.html#two-y-axes",
    "title": "Creating dual y-axis plots with ggplot2",
    "section": "Two y-axes",
    "text": "Two y-axes\nOne place to look for inspiration regarding plots is The R Graph Gallery. This wonderful website also has an example for creating a plot with a dual y-axis. Let’s walk through the process with our storm data. We’ll start by looking at the data we plan to plot.\n\nmod_storms %&gt;% \n  select(date_hour, wind, pressure) %&gt;% \n  head()\n\n# A tibble: 6 × 3\n  date_hour            wind pressure\n  &lt;dttm&gt;              &lt;int&gt;    &lt;int&gt;\n1 1975-06-27 00:00:00    25     1013\n2 1975-06-27 06:00:00    25     1013\n3 1975-06-27 12:00:00    25     1013\n4 1975-06-27 18:00:00    25     1013\n5 1975-06-28 00:00:00    25     1012\n6 1975-06-28 06:00:00    25     1012\n\n\nWe can see that there is quite a difference in the magnitude of the numeric values for wind and pressure. The approach for creating a secondary y-axis is somewhat artificial, and what we actually need to do is to scale one of the y-axis columns to have a similar range with the other column. Let’s start by finding the maximum values for wind and pressure.\n\nmax_wind &lt;- max(mod_storms$wind)\n\nmax_pressure &lt;- max(mod_storms$pressure)\n\n# scale wind values\nmod_storms2 &lt;- mod_storms %&gt;% \n  select(date_hour, wind, pressure) %&gt;% \n  mutate(scaled_wind1 = wind/max_wind,\n         scaled_wind2 = max_pressure*wind/max_wind, .after = wind)\n\nhead(mod_storms2)\n\n# A tibble: 6 × 5\n  date_hour            wind scaled_wind1 scaled_wind2 pressure\n  &lt;dttm&gt;              &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;int&gt;\n1 1975-06-27 00:00:00    25        0.417         422.     1013\n2 1975-06-27 06:00:00    25        0.417         422.     1013\n3 1975-06-27 12:00:00    25        0.417         422.     1013\n4 1975-06-27 18:00:00    25        0.417         422.     1013\n5 1975-06-28 00:00:00    25        0.417         422.     1012\n6 1975-06-28 06:00:00    25        0.417         422.     1012\n\n\nNow we have scaled the wind values to have a maximum value of 1 and also to have the same largest value as the pressure values. We could have also scaled the pressure values as well. The choice was arbitrary. Now we can see that the values can be easily plotted on the same scale.\n\n# plot wind and pressure\nmod_storms2  %&gt;% \n  pivot_longer(cols = c(\"scaled_wind2\", \"pressure\"), names_to = \"phenomenon\", values_to = \"measurement\") %&gt;%\n  ggplot(aes(x = date_hour, y = measurement, lty = phenomenon)) +\n  geom_line() +\n  labs(x = \"\", y = \"Measurement value\")\n\n\n\n\nChanges in wind speed and air pressure during the Amy storm. Wind speed values have been scaled to match the maximum air pressure values.\n\n\n\n\nBoth lines are now sharing the same y-axis. Now all we need to do is to show the wind speed values on the secondary y-axis. However, there is one improvement we can do while we are at it. In the plot above, the wind speed values have been scaled to match the maximum value of air pressure. We can see that for this particular data this is not a great choice, as the relative changes in the data are so different for the two phenomena. We can fix this by changing the scaling so that the range of wind speed values matches the range of air pressure values.\nThe secondary y-axis can be introduced by using the sec_axis() function. We need to do an inverse scaling for the y-label values to display the original wind speeds.\n\n# rescale wind data between min and max of pressure\nmin_pressure &lt;- min(mod_storms$pressure)\nmin_wind &lt;- min(mod_storms$wind)\n\n# scale wind values\nmod_storms2 &lt;- mod_storms %&gt;% \n  select(date_hour, wind, pressure) %&gt;% \n  mutate(scaled_wind1 = (wind - min_wind)/(max_wind  - min_wind),\n         scaled_wind2 = (max_pressure - min_pressure) * scaled_wind1 + min_pressure, .after = wind)\n\n\nmod_storms2  %&gt;% \n  ggplot(aes(x = date_hour)) +\n  geom_line(aes(y = pressure)) + \n  geom_line(aes(y =  scaled_wind2), lty = 2, color = \"red\") + \n  scale_y_continuous(\n    # Features of the first axis\n    name = \"Pressure (mbar)\",\n    # Add a second axis and specify its features\n    sec.axis = sec_axis(\n        # inverse transformation of scaled wind speeds for secondary y-axis\n      ~ (. - min_pressure)*(max_wind - min_wind)/(max_pressure - min_pressure) + min_wind, \n      name = \"Wind speed (knots)\"\n      )\n  ) +\n  labs(x = element_blank())\n\n\n\n\nChanges in wind speed (red dashed line) and air pressure (black solid line) during the Amy storm. Wind speed values have been scaled to match the air pressure value range. The labels for the secondary y-axis have undergone inverse scaling to show the unchanged wind speed values.\n\n\n\n\nNow we can see both lines plotted in the same graph, and we have an individual y-axis to show independent values and units for the two phenomena. This makes it easy to compare the lines."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tero Jalkanen",
    "section": "",
    "text": "Bio\nI’m a data scientist working in a statistical consulting company. In this role, I help life science companies working on novel drugs, diagnostic tests, and alike, with any data-related challenges they might face. I’m trained as a physicist with specialization towards materials science. I also have extensive experience in working in the pharmaceutical industry.\n\n\nEducation\nPhD in Physics | University of Turku, Finland (2008-2012).\nWrote my thesis on nanostructured silicon optical filters.\nMSc in Physics | University of Turku, Finland (2002-2008).\nMinored in Mathematics.\n\n\nExperience\nSenior Data Scientist | EstiMates (2022-present)\nData science consulting for life science companies.\nSenior Scientist / Expert Scientist / Scientist | Bayer (2014-2022)\nDevelopment of long-acting drug delivery formulations, internal statistical consultation, project management etc.\nPost-Doctoral Researcher | University of Turku, Finland (2012-2014)\nIndependent research on nanomaterials at the department of Physics and Astronomy. Materials science related consulting for external companies. Teaching students in Physics Lab II & III.\nVisiting Researcher | Kyoto University, Japan (2010-2012)\nWorked at the Institute of Advanced Energy for 18 months with a scholarship from the Japanese Ministry of Education, Culture, Sports, Science and Technology."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Tero’s Blog",
    "section": "",
    "text": "Six Sigma tools in R\n\n\n\n\n\n\n\nSix Sigma\n\n\nprocess improvement\n\n\nR\n\n\n\n\nImproving processes with the help of R\n\n\n\n\n\n\nSep 29, 2023\n\n\nTero Jalkanen\n\n\n\n\n\n\n  \n\n\n\n\nUsing Docker for Shiny Apps\n\n\n\n\n\n\n\nDocker\n\n\nShiny\n\n\n\n\nContainerizing your Shiny apps\n\n\n\n\n\n\nMar 13, 2023\n\n\nTero Jalkanen\n\n\n\n\n\n\n  \n\n\n\n\nDimensionality reduction with Uniform Manifold Approximation and Projection\n\n\n\n\n\n\n\ndimensionality reduction\n\n\nUMAP\n\n\nPCA\n\n\n\n\nUsing UMAP with R\n\n\n\n\n\n\nFeb 7, 2023\n\n\nTero Jalkanen\n\n\n\n\n\n\n  \n\n\n\n\nCreating dual y-axis plots with ggplot2\n\n\n\n\n\n\n\nggplot2\n\n\ndataviz\n\n\n\n\nSometimes one y-axis just does not suffice\n\n\n\n\n\n\nJan 30, 2023\n\n\nTero Jalkanen\n\n\n\n\n\n\n  \n\n\n\n\nBuilding your website with Quarto\n\n\n\n\n\n\n\nblog\n\n\nquarto\n\n\n\n\nA brief story about how I built this website with Quarto\n\n\n\n\n\n\nJan 5, 2023\n\n\nTero Jalkanen\n\n\n\n\n\n\n  \n\n\n\n\nHello Blog\n\n\n\n\n\n\n\nblog\n\n\n\n\nWelcome to my blog\n\n\n\n\n\n\nDec 30, 2022\n\n\nTero Jalkanen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/01-hello-blog.html",
    "href": "posts/01-hello-blog.html",
    "title": "Hello Blog",
    "section": "",
    "text": "Hi there, and welcome to my blog!\nI used to write a blog years ago, when I was living abroad, and I quite liked it. After a long hiatus, I thought that it might be nice to give it a try once more.\nThis blog is intended mostly for myself, to help me clarify my thoughts around data science related topics. I have found that writing is a very helpful aid in learning things. If someone else also finds the topics interesting/useful, then that is a great bonus.\nWelcome aboard. Let’s see what lies ahead."
  },
  {
    "objectID": "posts/04-umap.html",
    "href": "posts/04-umap.html",
    "title": "Dimensionality reduction with Uniform Manifold Approximation and Projection",
    "section": "",
    "text": "Uniform Manifold Approximation and Projection, UMAP for short, is a method which can be used to reduce the number of dimensions in a data set. Dimensionality reduction methods can be very useful when you are dealing with a large number of features, and can help mitigate issues such as multicollinearity. Personally I like to use them during exploratory data analysis. The umap package provides means for using UMAP with R. Let’s explore the use of UMAP a bit and compare it to another dimensionality reduction technique, Principal Component Analysis (PCA)."
  },
  {
    "objectID": "posts/04-umap.html#using-the-built-in-data-swiss",
    "href": "posts/04-umap.html#using-the-built-in-data-swiss",
    "title": "Dimensionality reduction with Uniform Manifold Approximation and Projection",
    "section": "Using the built-in data swiss",
    "text": "Using the built-in data swiss\nLet’s test drive both UMAP and PCA by using the built-in swiss data, which contains different numeric indicators for 47 Swiss municipalities.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\ntheme_set(theme_minimal())\n\n# load the built-in data\ndata(swiss)\n\nglimpse(swiss)\n\nRows: 47\nColumns: 6\n$ Fertility        &lt;dbl&gt; 80.2, 83.1, 92.5, 85.8, 76.9, 76.1, 83.8, 92.4, 82.4,…\n$ Agriculture      &lt;dbl&gt; 17.0, 45.1, 39.7, 36.5, 43.5, 35.3, 70.2, 67.8, 53.3,…\n$ Examination      &lt;int&gt; 15, 6, 5, 12, 17, 9, 16, 14, 12, 16, 14, 21, 14, 19, …\n$ Education        &lt;int&gt; 12, 9, 5, 7, 15, 7, 7, 8, 7, 13, 6, 12, 7, 12, 5, 2, …\n$ Catholic         &lt;dbl&gt; 9.96, 84.84, 93.40, 33.77, 5.16, 90.57, 92.85, 97.16,…\n$ Infant.Mortality &lt;dbl&gt; 22.2, 22.2, 20.2, 20.3, 20.6, 26.6, 23.6, 24.9, 21.0,…\n\n\nWe can condense the variability found in the data into smaller dimensions by using PCA. Let’s see how."
  },
  {
    "objectID": "posts/04-umap.html#dimensionality-reduction-via-pca",
    "href": "posts/04-umap.html#dimensionality-reduction-via-pca",
    "title": "Dimensionality reduction with Uniform Manifold Approximation and Projection",
    "section": "Dimensionality reduction via PCA",
    "text": "Dimensionality reduction via PCA\nThere are many packages for performing PCA in R. For example, the {stats} package has the prcomp function. Let’s use it, and visualize the first two principal components.\n\n# Perform PCA\npca.out &lt;- prcomp(swiss, scale. = TRUE)\n\n# record names of municipalities into a tibble\nmunicipalities &lt;- row.names(pca.out$x) %&gt;% \n  as_tibble() %&gt;% \n  rename(municipality = value)\n\n# visualize first two principal components\npca.out$x %&gt;% \n  as_tibble() %&gt;% \n  bind_cols(municipalities) %&gt;% \n  ggplot(aes(x = PC1, y = PC2, label = municipality)) +\n  geom_point() +\n  geom_text(check_overlap = TRUE, nudge_y = -0.1) +\n  ggtitle(\"PCA output\")\n\n\n\n\nThe first two principal components of the swiss data.\n\n\n\n\nFrom the plot above we can see which municipalities share similarities based on the data. That being said, this example is somewhat artificial, and does not make much sense beyond testing PCA output. In a more pragmatic setting, we might be interested, for example, if some numeric features help us to classify the observations into different classes. Here, we might want to know if the other numeric indicators in the data are connected to e.g. Fertility.\n\n\n\n\n\nA histogram of fertility values in the swiss data.\n\n\n\n\nLet’s define 70 as the limit of high fertility and label municipalities above this threshold as High. This choice divides the observations into two approximately equal sized groups, but other than that is completely arbitrary. Let’s re-try PCA with the remaining numeric columns.\n\n# modified data with fertility classes\nswiss_mod &lt;- swiss %&gt;% \n  mutate(fert_class = if_else(Fertility &gt; 70, true = \"High\", false = \"Low\")) %&gt;% \n  select(-Fertility)\n\n# PCA without the Fertility column\npca_mod &lt;- prcomp(select(swiss_mod, -fert_class), scale. = TRUE)\n\n# visualize first two principal components for the modified data\npca_mod$x %&gt;% \n  as_tibble() %&gt;% \n  bind_cols(municipalities) %&gt;% \n  bind_cols(select(swiss_mod, fert_class)) %&gt;% \n  ggplot(aes(x = PC1, y = PC2, label = municipality, color = fert_class)) +\n  geom_point() +\n  geom_text(check_overlap = TRUE, nudge_y = -0.1) +\n  ggtitle(\"PCA output\", subtitle = \"Fertility divided into two classes\") +\n  scale_color_manual(values = brewer.pal(n = 3, \"Pastel1\")) +\n  labs(color = \"Fertility:\")\n\n\n\n\nThe first two principal components of the modified swiss data. Municipalities are marked as having High fertility if the numeric value exceeds 70.\n\n\n\n\nWe can see that the numeric columns provide some degree of separation between the high and low fertility municipalities."
  },
  {
    "objectID": "posts/04-umap.html#dimensionality-reduction-with-umap",
    "href": "posts/04-umap.html#dimensionality-reduction-with-umap",
    "title": "Dimensionality reduction with Uniform Manifold Approximation and Projection",
    "section": "Dimensionality reduction with UMAP",
    "text": "Dimensionality reduction with UMAP\nLet’s try the umap function on the modified swiss data to see what we get as output.\n\nlibrary(umap)\nset.seed(123)\n\n# calculate UMAP for the data\numap.out &lt;- umap(select(swiss_mod, -fert_class))\n\n# plot the two UMAP components\numap.out$layout %&gt;% \n  as.data.frame() %&gt;% \n  rename(UMAP1 = V1, UMAP2 = V2) %&gt;% \n  bind_cols(municipalities) %&gt;% \n  bind_cols(select(swiss_mod, fert_class)) %&gt;% \n  ggplot(aes(x = UMAP1, y = UMAP2, label = municipality, color = fert_class)) +\n  geom_point() +\n  geom_text(check_overlap = TRUE, nudge_y = -0.1) +\n  ggtitle(\"UMAP output\") +\n  scale_color_manual(values = brewer.pal(n = 3, \"Pastel1\"))  +\n  labs(color = \"Fertility:\")\n\n\n\n\nThe first two UMAP components for the modified swiss data.\n\n\n\n\nWe can see that the output from UMAP is somewhat different. The observations are more clearly divided into two clusters. The clusters do not seem to fully overlap with the Fertility classes we constructed.\nIt is good to understand that the UMAP algorithm contains a random element, which means that the output of the algorithm differs slightly depending on the random seed. Let’s test this by running the calculations again with a different seed.\n\nset.seed(345)\n\n\n\n\n\n\nRe-calculated UMAP components with a different random seed.\n\n\n\n\nWe notice that the output is quite similar, but still slightly different. There are actually quite a few arguments we can feed to the umap function. The default values are listed under umap.defaults. Let’s take a look.\n\numap.defaults\n\numap configuration parameters\n\n\n           n_neighbors: 15\n\n\n          n_components: 2\n\n\n                metric: euclidean\n\n\n              n_epochs: 200\n\n\n                 input: data\n\n\n                  init: spectral\n\n\n              min_dist: 0.1\n\n\n      set_op_mix_ratio: 1\n\n\n    local_connectivity: 1\n\n\n             bandwidth: 1\n\n\n                 alpha: 1\n\n\n                 gamma: 1\n\n\n  negative_sample_rate: 5\n\n\n                     a: NA\n\n\n                     b: NA\n\n\n                spread: 1\n\n\n          random_state: NA\n\n\n       transform_state: NA\n\n\n                   knn: NA\n\n\n           knn_repeats: 1\n\n\n               verbose: FALSE\n\n\n       umap_learn_args: NA\n\n\nBy changing these default values we can affect the output. Let’s see what happens when we change the default number of components and nearest neighbors.\n\n# re-calculate UMAP for the data\numap.out &lt;- umap(select(swiss_mod, -fert_class),\n                 n_components = 4, \n                 random_state = 333,\n                 n_neighbors = 5)\n\n\n\n\n\n\nRe-calculated first and second UMAP components with a different random seed, more components and smaller number of nearest neighbors.\n\n\n\n\nNow the output resembles more the one we got by using PCA. The original article on UMAP goes into finer detail with regards to the effects of hyperparameter tuning, but we can certainly see that we can tune the output significantly."
  },
  {
    "objectID": "posts/06-SixSigma.html",
    "href": "posts/06-SixSigma.html",
    "title": "Six Sigma tools in R",
    "section": "",
    "text": "Six Sigma (\\(6 \\sigma\\)) is an umbrella term for a set of tools and techniques, which can be used for improving existing processes for example in the manufacturing industry. There are many commercial software packages which offer Six Sigma related statistical quality control tools for analyzing data. Recently I bumped into the Six Sigma R package, which provides a convenient open source alternative. Let’s take a look at a few functions supplied by this package."
  },
  {
    "objectID": "posts/06-SixSigma.html#methodology-for-improving-an-existing-process",
    "href": "posts/06-SixSigma.html#methodology-for-improving-an-existing-process",
    "title": "Six Sigma tools in R",
    "section": "Methodology for improving an existing process",
    "text": "Methodology for improving an existing process\nSix Sigma projects often revolve around the DMAIC methodology, which stands for Define, Measure, Analyze, Improve, and Control. These five stages are integral to a Six Sigma project which aims at improving an existing process. Each stage contains a specific set of tools which are used for improving the overall quality.\nLet’s assume that we have defined our project aim as improving the reproducibility of a sawmill production process, where we produce lumber with two different machines which are run by three different machine operators. The SixSigma package provides a bunch of handy tools for analyzing the situation. We will take a closer look at the Gage R&R method and the Ishikawa or fishbone diagrams for measuring and analyzing the project data. Let’s start by loading the needed packages.\n\nlibrary(SixSigma)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/06-SixSigma.html#creating-mock-data",
    "href": "posts/06-SixSigma.html#creating-mock-data",
    "title": "Six Sigma tools in R",
    "section": "Creating mock data",
    "text": "Creating mock data\nBefore we can analyze anything, we need to create some mock data. Let’s assume that the target of the sawmill cutting process is to produce 24 feet long pieces of lumber. In our imaginary situation, we have the three operators cut 5 pieces of lumber per machine, which we then measure to analyze the consistency of the length of lumber produced.\n\nset.seed(123)\n\nmachines &lt;- c(\"m #1\", \"m #2\")\noperators &lt;- c(\"op #1\", \"op #2\", \"op #3\")\nruns &lt;- c(\"run #1\", \"run #2\", \"run #3\", \"run #4\", \"run #5\")\n\n# create basic measurement grid\nlumber_df &lt;- expand.grid(\"machine\" = factor(x = machines, levels = machines),\n                         \"operator\" = factor(x = operators, operators),\n                         \"run\" = factor(x = runs, levels = runs))\n\n# generate measurement data\nlumber_df$lumber_length &lt;- rnorm(n = length(lumber_df$machine), mean = 24, sd = 0.03)\n\n# let's create some differences\nlumber_df &lt;- lumber_df |&gt;\n  mutate(\n    lumber_length = case_when(\n      # machine 2 creates slightly longer products\n      machine == \"m #2\" ~ lumber_length + lumber_length * 0.01,\n      # operator 3 has slighty more variation in the outcomes\n      operator == \"op #3\" ~ lumber_length + rnorm(n = 1, mean = 0, sd = 0.035),\n      # keep other values as is\n      TRUE ~ lumber_length\n    )\n  )\n\nhead(lumber_df)\n\n  machine operator    run lumber_length\n1    m #1    op #1 run #1      23.98319\n2    m #2    op #1 run #1      24.23303\n3    m #1    op #2 run #1      24.04676\n4    m #2    op #2 run #1      24.24214\n5    m #1    op #3 run #1      24.01880\n6    m #2    op #3 run #1      24.29197"
  },
  {
    "objectID": "posts/06-SixSigma.html#using-gage-rr",
    "href": "posts/06-SixSigma.html#using-gage-rr",
    "title": "Six Sigma tools in R",
    "section": "Using Gage R&R",
    "text": "Using Gage R&R\nGage R&R is used to evaluate the repeatability and reproducibility of the lumber cutting process. It helps us measure the performance of the machines and operators. For example, we can determine how much of the variation in the lumber length is due to the sawing machines itself, and how much is due to the actual process variation. The SixSigma package provides us with a convenient function ss.rr() for performing the analysis. Let’s imagine that we wish to keep the lumber within 0.1 feet from the target value. Figure 1 shows the visualization of the Gage R&R results.\n\nss.rr(var = lumber_length, machine, operator, data = lumber_df, \n    sub = \"Lumber length analysis\", \n    alphaLim = 0.05,\n    errorTerm = \"interaction\",\n    lsl = 24 - 0.1,\n    usl = 24 + 0.1,\n    method = \"crossed\")\n\nComplete model (with interaction):\n\n                 Df Sum Sq Mean Sq F value  Pr(&gt;F)\nmachine           1 0.4022  0.4022 651.851 0.00153\noperator          2 0.0018  0.0009   1.451 0.40802\nmachine:operator  2 0.0012  0.0006   0.651 0.53032\nRepeatability    24 0.0227  0.0009                \nTotal            29 0.4279                        \n\nalpha for removing interaction: 0.05 \n\n\nReduced model (without interaction):\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)\nmachine        1 0.4022  0.4022 436.277 &lt;2e-16\noperator       2 0.0018  0.0009   0.971  0.392\nRepeatability 26 0.0240  0.0009               \nTotal         29 0.4279                       \n\nGage R&R\n\n                       VarComp %Contrib\nTotal Gage R&R    0.0009218159     3.33\n  Repeatability   0.0009218159     3.33\n  Reproducibility 0.0000000000     0.00\n    operator      0.0000000000     0.00\nPart-To-Part      0.0267496612    96.67\nTotal Variation   0.0276714771   100.00\n\n                       VarComp     StdDev  StudyVar %StudyVar %Tolerance\nTotal Gage R&R    0.0009218159 0.03036142 0.1821685     18.25      91.08\n  Repeatability   0.0009218159 0.03036142 0.1821685     18.25      91.08\n  Reproducibility 0.0000000000 0.00000000 0.0000000      0.00       0.00\n    operator      0.0000000000 0.00000000 0.0000000      0.00       0.00\nPart-To-Part      0.0267496612 0.16355324 0.9813194     98.32     490.66\nTotal Variation   0.0276714771 0.16634746 0.9980848    100.00     499.04\n\nNumber of Distinct Categories = 7 \n\n\n\n\n\nFigure 1: Gage R&R diagram showing that machine #2 produces products which are clearly above the target range.\n\n\n\n\nWe can see that the function spits out a bunch of statistical results and a fairly nice looking visualization of the main results. Our main takeaway from the outputs is that machine #2 has a statistically significant effect on the length of the boards. Otherwise, it seems that the operators do not have a noticeable impact on the process outcome, which is an indicator of a robust production process."
  },
  {
    "objectID": "posts/06-SixSigma.html#analyzing-the-potential-contributors-to-lumber_length",
    "href": "posts/06-SixSigma.html#analyzing-the-potential-contributors-to-lumber_length",
    "title": "Six Sigma tools in R",
    "section": "Analyzing the potential contributors to lumber_length",
    "text": "Analyzing the potential contributors to lumber_length\nThe fishbone or Ishikawa diagram can be used to write down and organize the possible causes for the performance differences between the machines and operators. In this case, the difference between the machines was quite obvious. However, we might assume that other factors such as the operators, the type of wood used in cutting, the tool used for measuring the lengths of the boards, and possibly the environment we work in also play a role in the outcome. The fishbone diagram is a convenient graph for visualizing this type of data. We can plot the contributors by using the ss.ceDiag() function.\nFigure 2 shows the results of our brainstorming session. We now have a list of possible contributors to the process, which can help us plan the project further.\n\neffect &lt;- \"Board length\"\ncauses.gr &lt;- c(\"Machine\", \"Operator\", \"Wood type\", \"Environment\", \"Measurement tool\")\ncauses &lt;- vector(mode = \"list\", length = length(causes.gr))\n\n# possible contributors for different groups\n\n# machine\ncauses[1] &lt;- list(c(\"calibration\", \"blade\", \"conveyor belt\"))\n# operator\ncauses[2] &lt;- list(c(\"operator #1\", \"operator #2\", \"operator #3\"))\n# wood type\ncauses[3] &lt;- list(c(\"tree species\", \"hardness\", \"thickness\"))\n# environment\ncauses[4] &lt;- list(c(\"humidity\", \"cleanliness\"))\n# measurement tool\ncauses[5] &lt;- list(c(\"calibration\", \"producer\", \"type\"))\n\nss.ceDiag(effect, causes.gr, causes, sub = \"Possible contributors to board length\")\n\n\n\n\nFigure 2: Fishbone diagram showing possible contributors to the length of boards in a sawmill production process."
  },
  {
    "objectID": "posts/06-SixSigma.html#final-thoughts",
    "href": "posts/06-SixSigma.html#final-thoughts",
    "title": "Six Sigma tools in R",
    "section": "Final thoughts",
    "text": "Final thoughts\nThe SixSigma package provides an open source alternative for common statistical and project planning tools used in the DMAIC methodology. The functions are well documented and easy to use. Furthermore, the plots provided by the package functions seem nice enough to include directly in reports. Give the package a go if you need to improve processes in a Six Sigma fashion."
  },
  {
    "objectID": "posts/starting-a-website/start-a-website.html",
    "href": "posts/starting-a-website/start-a-website.html",
    "title": "Building your website with Quarto",
    "section": "",
    "text": "This website is built with Quarto, which is an open-source publishing system. It can be used for making reports, books, presentations, websites etc. I wanted to learn how to use Quarto for making a website, so I decided to make one for myself. In this blog post I’ll go trough some the basics which are needed for creating a simple website."
  },
  {
    "objectID": "posts/starting-a-website/start-a-website.html#committing-the-files",
    "href": "posts/starting-a-website/start-a-website.html#committing-the-files",
    "title": "Building your website with Quarto",
    "section": "Committing the files",
    "text": "Committing the files\nGo to RStudio git-pane:\n\n\n\n\n\nClick of the Staged tick boxes in the git pane and push commit. A pop-up pane will appear. Write a brief commit message and push the Commit button:\n\n\n\n\n\nNow let’s try to sync the repos again. Push the button with purple squares again and push Create in the pop-up menu to sync the repos:\n\n\n\n\n\nYou will get a warning message. Push Overwrite and sync the repos:\n\n\n\n\n\nYou’re done:\n\n\n\n\n\nWhen you now visit your github repo at https://github.com/your-username/your-repo-name, you should find all the files you committed there. From now on, you should be able to update any local changes to your GitHub-repo by committing and pushing from the Git-pane."
  }
]