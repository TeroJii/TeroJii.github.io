---
title: "Dimensionality reduction with Uniform Manifold Approximation and Projection"
date: "2023-02-07"
categories: [dimensionality reduction, UMAP]
description: 'Using UMAP with R'
draft: true
---

Uniform Manifold Approximation and Projection, [UMAP](https://arxiv.org/abs/1802.03426) for short, is a method which can be used to reduce the number of dimensions in a data set. Dimensionality reduction methods can be very useful when you are dealing with a large number of features, and can help mitigate issues such as multicollinearity. The [umap](https://cran.r-project.org/web/packages/umap/vignettes/umap.html) package provides means for using UMAP with R. Let's explore the use of UMAP a bit and compare it to another dimensionality reduction technique, Principal Component Analysis (PCA).

## Using the built-in data `swiss`

Let's test drive both UMAP and PCA by using the built-in `swiss` data, which contains different numeric indicators for 47 Swiss municipalities.

```{r}
library(tidyverse)
theme_set(theme_minimal())

# load the built-in data
data(swiss)

glimpse(swiss)
```

We can condense the variability found in the data into smaller dimensions by using PCA. Let's see how.

## Dimensionality reduction via PCA

There are many packages for performing PCA in R. For example, the {stats} package has the `prcomp` function. Let's use it, and visualize the first two principal components.
 
```{r}
#| fig-cap: "The first two principal components of the `swiss` data."

# Perform PCA
pca.out <- prcomp(swiss, scale. = TRUE)

# record names of municipalities into a tibble
municipalities <- row.names(pca.out$x) %>% 
  as_tibble() %>% 
  rename(municipality = value)

# visualize first two principal components
pca.out$x %>% 
  as_tibble() %>% 
  bind_cols(municipalities) %>% 
  ggplot(aes(x = PC1, y = PC2, label = municipality)) +
  geom_point() +
  geom_text(check_overlap = TRUE, nudge_y = -0.1) +
  ggtitle("PCA output")

```

Now this example is somewhat artificial, and does not make much sense beyond testing PCA output. In a more pragmatic setting we might be interested, for example, if some numeric features help us to classify the observations into different classes. Here, we might want to know if the other numeric indicators in the data are connected to `Fertility`.

```{r}
#| fig-cap: "A histogram of fertility values in the swiss data."
#| warning: false
#| echo: false
library(RColorBrewer)

swiss %>% 
  ggplot(aes(x = Fertility, fill = Fertility > 70)) +
  geom_histogram(color = "black", 
                 binwidth = 5) +
  scale_x_continuous(limits = c(30,100)) +
  scale_fill_manual(values = brewer.pal(n = 3, "Pastel1"))

```

Let's define `70` as the limit of high fertility and label municipalities above this threshold as `High`. This choice divides the observations into two approximately equal sized groups, but other than that is completely random. Let's re-try PCA with the remaining numeric columns.

```{r}
#| fig-cap: "The first two principal components of the modified `swiss` data. Municipalities are marked as having High fertility if the numeric value exceeds 70."

# modified data with fertility classes
swiss_mod <- swiss %>% 
  mutate(fert_class = if_else(Fertility > 70, true = "High", false = "Low")) %>% 
  select(-Fertility)

# PCA without the Fertility column
pca_mod <- prcomp(select(swiss_mod, -fert_class), scale. = TRUE)

# visualize first two principal components for the modified data
pca_mod$x %>% 
  as_tibble() %>% 
  bind_cols(municipalities) %>% 
  bind_cols(select(swiss_mod, fert_class)) %>% 
  ggplot(aes(x = PC1, y = PC2, label = municipality, color = fert_class)) +
  geom_point() +
  geom_text(check_overlap = TRUE, nudge_y = -0.1) +
  ggtitle("PCA output", subtitle = "Fertility divided into two classes") +
  scale_color_manual(values = brewer.pal(n = 3, "Pastel1")) +
  labs(color = "Fertility:")

```

We can see that the numeric columns provide some degree of separation between the high and low fertility municipalities.

## Dimensionality reduction with UMAP

Let's try the `umap` function on the modified `swiss` data to see what we get as output.

```{r}
#| fig-cap: "The first two UMAP components for the modified `swiss` data."

library(umap)
set.seed(123)

# calculate UMAP for the data
umap.out <- umap(select(swiss_mod, -fert_class))

# plot the two UMAP components
umap.out$layout %>% 
  as.data.frame() %>% 
  rename(UMAP1 = V1, UMAP2 = V2) %>% 
  bind_cols(municipalities) %>% 
  bind_cols(select(swiss_mod, fert_class)) %>% 
  ggplot(aes(x = UMAP1, y = UMAP2, label = municipality, color = fert_class)) +
  geom_point() +
  geom_text(check_overlap = TRUE, nudge_y = -0.1) +
  ggtitle("UMAP output") +
  scale_color_manual(values = brewer.pal(n = 3, "Pastel1"))  +
  labs(color = "Fertility:")

```

We can see that the output from UMAP is somewhat different from PCA. The observations are clearly divided into two clusters. The clusters do not seem to fully overlap with the Fertility classes we constructed.

It is good to understand that the UMAP algorithm contains a random element, which means that the output of the algorithm differs slightly depending on the random seed. Let's test this by running the calculations again with a different seed.

```{r}
set.seed(345)

```


```{r}
#| fig-cap: "Re-calculated UMAP components with a different random seed."
#| echo: false

# re-calculate UMAP for the data
umap.out <- umap(select(swiss_mod, -fert_class))

#plot output
umap.out$layout %>% 
  as.data.frame() %>% 
  rename(UMAP1 = V1, UMAP2 = V2) %>% 
  bind_cols(municipalities) %>% 
  bind_cols(select(swiss_mod, fert_class)) %>% 
  ggplot(aes(x = UMAP1, y = UMAP2, label = municipality, color = fert_class)) +
  geom_point() +
  geom_text(check_overlap = TRUE, nudge_y = -0.1) +
  ggtitle("UMAP output", subtitle = "With a different random seed") +
  scale_color_manual(values = brewer.pal(n = 3, "Pastel1")) +
  labs(color = "Fertility:")

```

We notice that the output is quite similar, but still slightly different. There are actually quite a few arguments we can feed to the `umap` function. The default values are listed under `umap.defaults`. Let's take a look.

```{r}
umap.defaults
```

By changing these default values we can affect the output. Let's see what happens when we change the default number of components and nearest neighbors.

```{r}
# re-calculate UMAP for the data
umap.out <- umap(select(swiss_mod, -fert_class),
                 n_components = 4, 
                 random_state = 333,
                 n_neighbors = 5)
```


```{r}
#| fig-cap: "Re-calculated first and second UMAP components with a different random seed, more components and smaller number of nearest neighbors."
#| echo: false

#plot output
umap.out$layout %>% 
  as.data.frame() %>% 
  rename(UMAP1 = V1, UMAP2 = V2) %>% 
  bind_cols(municipalities) %>% 
  bind_cols(select(swiss_mod, fert_class)) %>% 
  ggplot(aes(x = UMAP1, y = UMAP2, label = municipality, color = fert_class)) +
  geom_point() +
  geom_text(check_overlap = TRUE, nudge_y = -0.1) +
  ggtitle("UMAP output",
          subtitle = "With modified parameters") +
  scale_color_manual(values = brewer.pal(n = 3, "Pastel1")) +
  labs(color = "Fertility:")

```

Now the output resembles more the one we got by using PCA. The  [original article on UMAP](https://arxiv.org/pdf/1802.03426.pdf) goes into finer detail with regards to the effects of hyperparameter tuning, but we can certainly see that we can tune the output significantly. 

# Final thoughts

The {umap} package makes it easy to use UMAP in R. Although we barely scratched the surface here, it was interesting to see how UMAP gives different outputs compared to PCA.

If you are interested in using UMAP as a part of your modeling pipeline, you can also find it from [tidymodels](https://www.tmwr.org/dimensionality.html). Julia Silge has a nice [blog post](https://juliasilge.com/blog/cocktail-recipes-umap/) comparing the use of PCA and UMAP when using {tidymodels}.
